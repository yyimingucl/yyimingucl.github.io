<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: March 18, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Yiming Yang"><meta name=description content="Overview In regression problem, we are actually looking for a function that maps input $x$ to output $y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,&mldr;) and optimize their weights to approximate the true function."><link rel=alternate hreflang=en-us href=https://yyimingucl.github.io/post/gpr/><link rel=canonical href=https://yyimingucl.github.io/post/gpr/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://yyimingucl.github.io/post/gpr/featured.png"><meta property="og:site_name" content="Yiming Yang"><meta property="og:url" content="https://yyimingucl.github.io/post/gpr/"><meta property="og:title" content="From gaussian process back to linear regression | Yiming Yang"><meta property="og:description" content="Overview In regression problem, we are actually looking for a function that maps input $x$ to output $y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,&mldr;) and optimize their weights to approximate the true function."><meta property="og:image" content="https://yyimingucl.github.io/post/gpr/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-10-13T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-13T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://yyimingucl.github.io/post/gpr/"},"headline":"From gaussian process back to linear regression","image":["https://yyimingucl.github.io/post/gpr/featured.png"],"datePublished":"2022-10-13T00:00:00Z","dateModified":"2022-10-13T00:00:00Z","author":{"@type":"Person","name":"Yiming Yang"},"publisher":{"@type":"Organization","name":"Yiming Yang","logo":{"@type":"ImageObject","url":"https://yyimingucl.github.io/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_192x192_fill_lanczos_center_3.png"}},"description":"Overview In regression problem, we are actually looking for a function that maps input $x$ to output $y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,\u0026hellip;) and optimize their weights to approximate the true function."}</script><title>From gaussian process back to linear regression | Yiming Yang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=507055da84bcaa46720eb2b80d881b3e><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Yiming Yang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Yiming Yang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#four-legged_partners><span>Four-legged families</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>From gaussian process back to linear regression</h1><div class=article-metadata><div><span class=author-highlighted>Yiming Yang</span></div><span class=article-date>Oct 13, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:1200px;max-height:500px><div style=position:relative><img src=/post/gpr/featured_hu226ec8eb1d16adda9577568b9d21df1f_629004_1200x2500_fit_q75_h2_lanczos_3.webp width=1200 height=500 alt class=featured-image></div></div><div class=article-container><div class=article-style><h1 id=overview>Overview</h1><p>In regression problem, we are actually looking for a function that maps input
$x$ to output
$y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,&mldr;) and optimize their weights to approximate the true function. The problem with this approach is that we need to decide what kind of functions to use, and if the chosen type of function does not naturally match the underlying function, we can never obatin a well-apprxoimated function by only adjusting the weights. 2. The second considers arbitrary functions and chooses the one that fits the given sample
$(X, Y)$ more closely (greater likelihood). The problem with this method is that it needs to consider an infinite number of functions, but this is not possible and thus requires the use of the Gaussian process in the title.</p><p>Firstly, I will give the definition of <a href=https://en.wikipedia.org/wiki/Gaussian_process target=_blank rel=noopener>Gaussian Process</a> from wikipedia: A Gaussian process is a stochastic process (a collection of random variables indexed by time or space) such that every finite collection of those random variables forms a multivariate normal distribution. Remark: <strong>Every finite collection of those random variables forms a multivariate normal distribution</strong>. This property will be our panacea for any upcoming troubles!
correlation</p><h3 id=1-two-dimensional-gaussian-distribution>1. Two-dimensional Gaussian distribution</h3><p>From the above definition, it is clear that any number of random variables form a Gaussian distribution. To simplify the problem, we firstly take two of these random variables such that
$(x_1,x_2)\sim\mathcal{N}(\mu,\Sigma)$. Three pictures below: (left) Density contours of the distribution. (right) Sampled values of the random variables
$x_1,x_2$.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_3747dd93f11ee0bd224a6f8f8f786882.webp 400w,
/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_e479320415b21d1605f06ad243e31cb6.webp 760w,
/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_3747dd93f11ee0bd224a6f8f8f786882.webp width=608 height=353 loading=lazy data-zoomable></div></div></figure><em>cov(x1,x2)=0</em><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_e73a2b196d631caf75fc76a53b058ca8.webp 400w,
/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_cf0c856d4472984b4a43237d0404f514.webp 760w,
/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_e73a2b196d631caf75fc76a53b058ca8.webp width=608 height=353 loading=lazy data-zoomable></div></div></figure><em>cov(x1,x2)=0.7</em><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c8ad0e440fc78778d04f3144c4f9b0cd.webp 400w,
/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c6fc88e86678dabdb3d3e319caf0fcfd.webp 760w,
/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c8ad0e440fc78778d04f3144c4f9b0cd.webp width=608 height=353 loading=lazy data-zoomable></div></div></figure><em>cov(x1,x2)=0.95</em></p><p>As you can see from the graph above, as the correlation between
$x_1,x_2$ gets larger, the values of
$x_1$ and
$x_2$ that we sample become more and more similar. (As can be expected, when the correlation is close to 1, no matter how many times we sample,
$x_1$ is always equal to
$x_2$.</p><h3 id=2-high-dimensional-gaussian-distribution>2. High-dimensional Gaussian distribution</h3><p>The simple two-dimensional case was studied, let&rsquo;s now extend to 20 dimensions.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_9a21ff4f7208871dbca5533da46ca197.webp 400w,
/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_aed4ff7f6dbca320452b6dadfe614fd3.webp 760w,
/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_9a21ff4f7208871dbca5533da46ca197.webp width=760 height=247 loading=lazy data-zoomable></div></div></figure><em>20-dimensional Gaussian distribution</em></p><p>The two left figures above, same as in 2D case, are sampled values for
$(x_1,x_2,...x_{20})$ obtained from a 20-dimensional Gaussian distribution (Do the shown curves look like the non-linear regressions?) The right panel shows the covariance matrix, where you can see that variables has a strong correlation with their neighbors (also reflected in the two panes on the left, where the adjacent variables do not vary largely, thus making the whole curve very smooth.)</p><p>What if we fix two random variables and sample again?<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_3764b327f86795c728138c48652c72d2.webp 400w,
/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_54b4732790393aa3287fe5d344e9cb64.webp 760w,
/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_3764b327f86795c728138c48652c72d2.webp width=608 height=307 loading=lazy data-zoomable></div></div></figure>What if we viewed the two given random variables as given samples in the regression problem? The graph above can be seen as generating four curves that exactly fit the sample. Using the idea of averaging, we sample many curves and then take the average as our regression curve, but this is costly. Luckily, recalling the previous definition of a Gaussian process, the 20 random variables follow a twenty-dimensional Gaussian distribution, and their conditional probability distribution
$(x_{3:20}|x_1,x_2\sim Gaussian)$ remains a Gaussian distribution according to the properties of the Gaussian distribution.</p><table><thead><tr><th style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_b1339589bc5fcd05a732794381e78755.webp 400w,
/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_3810a2271173a8af29ba23f96ff14088.webp 760w,
/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_b1339589bc5fcd05a732794381e78755.webp width=370 height=248 loading=lazy data-zoomable></div></div></figure></th></tr></thead><tbody><tr><td style=text-align:center><em>Variance of each random variable (since the first two variables are already given, they can be seen as constants with a variance of zero. In the mean time, the adjacent variabes have a small variance due to the high correlation with the two variables.)</em></td></tr></tbody></table><p><strong>New Question: The above procedure only discusses the discrete case (integer index dimension), but the actual regression problems are often the continuous case.</strong> It is a straightforward idea to sample many times at infinite points (1000 dimensions, 10000 dimensions, &mldr;, and infinite dimensions) around the given sample to approximate a continuous function. However, this approach is extremely ineffective and impossible in fact.</p><h3 id=3-gaussian-process-regression>3. Gaussian Process Regression</h3><p>Recall from the previous definition of a Gaussian process that <strong>any number of random variables constitutes a Gaussian distribution</strong>. Generally speaking, if we take an infinite number of random variables will form an infinite-dimensional Gaussian distribution (infinite-dimensional vector of means, infinite-dimensional * infinite-dimensional covariance matrix). And further, if we consider each function as a very very long vector (an infinite-dimensional vector), then the two parameters of the infinite-dimensional Gaussian distribution, the mean and the variance, can be represented by two functions. The entire Gaussian process can then be written in the form:
$f(\cdot)\sim\mathcal{N}(m(\cdot), K(\cdot,\cdot))$ where
$m(\cdot)\,\,,K(\cdot,\cdot)$ are called the mean function and covariance function respectively. By definition in this way, we can get ride of the limitation of the discrete case and the mean and covariance matrix can be calculated for any function
$f(x)$.</p><p>From this form, it is possible to view the whole Gaussian process as sampling from a Gaussian distribution defined over functions (functional). Like the finite-dimensional Gaussian distribution, it is uniquely determined by the mean and covariance. Recalling the second approach to solving regression mentioned in the beginning, the Gaussian process does take into account all possible functions. Sampling only from this above form has a very low probability of sampling functions that match the sample points. Therefore, if we wish to obtain functions that match the sample, we need to combine the Gaussian process with given samples. From a Bayesian perspective, it is then possible to think of the Gaussian process as a prior distribution over functions. After combining with the given samples, we get the posterior distribution over functions.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_4cb7dc7eedcd74263e1925838a09ca47.webp 400w,
/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_1c1aea4b4a43d3d43ec8b71d775436af.webp 760w,
/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_4cb7dc7eedcd74263e1925838a09ca47.webp width=370 height=264 loading=lazy data-zoomable></div></div></figure><em>m( . )=0 and K( . , .) is gaussian kernel with alpha=2,beta=0.1</em></p><p>The figure above shows the 5 functions sampled from the prior distribution of this function. The blue region is
$\mu\pm\sigma^2$.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_df1a773d22f2e5ffa1212036f783908f.webp 400w,
/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_cc9c723220ba9373755f149f00973496.webp 760w,
/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_df1a773d22f2e5ffa1212036f783908f.webp width=370 height=248 loading=lazy data-zoomable></div></div></figure>The above figure demonstrates the functions sampled in the posterior distribution, given three samples
$(x_1,y_1),(x_2,y_2),(x_3,y_3)$. The thick black line is the mean. It can be seen that, same as in the second part of the 20-dimensional discrete case, the variance around the sample points is almost zero. (Another way to understand the process of combing samples is to reject the functions that do not match these sample points.)</p><h3 id=4-covariance-function-kernel>4. Covariance Function (Kernel)</h3><p>For given sample
$X=((x_1),(x_2),...,(x_n)$ (each column is the feature for each sample
$(x_i)$, then the posterior distribution can be written in the form
$f(X)\sim\mathcal{N}(m(X),K(X,X)$ where
$[K(X,X)]_{ij}=K(x_i,x_j)$.</p><p>Simply starting from the idea of the kernel trick, it is equivalent to quantifies the relationship between points on the feature space induced by the covariance function (though of course the choice of covariance function varies for different situations). Returning to the perspective of the infinite dimensional Gaussian distribution, the covariance function quantifies the relationship between infinitely closed points. In some sense, the covariance function
$K(\cdot\,\,,\cdot)$ determines the overall shape of the function (from an a priori perspective, the covaraince function expresses the priori knowledge of the desired function).</p><p>The covariance function and the choice of its hyperparameters is a really big topic (all the images above use the squared-exponential covariance function
$K(x_i,x_j)=\alpha\exp{-\frac{1}{2l^2}( x_i-x_j)^2}$). The squared-exponential covariance function is used here as an example for a brief discussion.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_e48cc6091b698651e8fda2ee7aeada97.webp 400w,
/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_23462f43502174721c04e8a2075ffe89.webp 760w,
/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_e48cc6091b698651e8fda2ee7aeada97.webp width=760 height=512 loading=lazy data-zoomable></div></div></figure>The above figure shows when the hyperparameter L in the kernel is small, the correlation between points is smale , and the neighbored points are limited to vary in a small range. (when using this kernel as a priori covariance function, we can imagine that the sampled function will exhibit very large fluctuations). Conversely, when the hyperparameter L is large, one specific point is still correlated with very distant points (The sampled function will be very smooth), and the following figure verifies these descriptions.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_e3fa0fe4b55a62493efd227452f7898d.webp 400w,
/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_078d5070fca6a837e843d92106d91ec1.webp 760w,
/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_e3fa0fe4b55a62493efd227452f7898d.webp width=760 height=471 loading=lazy data-zoomable></div></div></figure><em>Sample from Gaussian Process by squared-exponential covariance function with different parameter L</em></p><h3 id=5-use-gp-for-regression>5. Use GP for regression</h3><p>Summarizing the above process, a prior is first given for all potential functions by Gaussian process, then, combined with the samples, we obtain the posterior distribution for the function
$f(X)\sim\mathcal{N}(m(X),K(X,X)$ where
$[K(X,X)]_{ij}=K(x_i,x_j)$.</p><p>Taken the noise into account, the output function becomes
$y(X)=f(X)+\sigma^2_yI\,\,,\sigma^2_yI\sim\mathcal{N}(0,1)$ and
$y(X)\sim\mathcal{N}(m(X),K(X,X)+\sigma^2_y)$.</p><p><strong>New Question Again: How can we store an infinite dimensional Gaussian distribution in a computer with finite memory? Similarly, for the posterior distribution given samples
$(X,Y)$, we can see that the parameters are only finite dimensional (the mean is a vector of length n and the covariance matrix is n*n), so does this result contradict or behave inconsistently with the infinite dimensionality?</strong></p><h3 id=6-consistency-and-marginalisation-property>6. Consistency and Marginalisation Property</h3><p>Both of these problems can be solved perfectly by the <strong>Marginalisation Property</strong> of the Gaussian distribution. For Multivariate Gaussian distribution:
$$P(Y_1) = \int_{Y_2}P(Y_1,Y_2)dY_2\\ P(Y_1,Y_2) \sim N( \left(\begin{matrix} a\\ b\end{matrix}\right),\left(\begin{matrix} A &B\\ 	B^T&C \end{matrix}\right))\Rightarrow P(Y_1)\sim N(a, A)$$
With this property means that we can split only the part of our interest (samples and predictions) from the entire infinite dimensional Gaussian distribution and ignore these parts are not in our interest (no need to interpolation/extrapolation). This property is then used to derive for the distribution of our part of interest. 1. For the first problem, we just need to keep the multivariate Gaussian distribution of the sample part in the computer. 2. For the second problem, it can be seen from the above properties that the parameters of the marginal distribution obtained by partitioning the whole Gaussian distribution are also part of the parameters of the whole Gaussian distribution (or saying derived from the parameters of the infinite-dimensional distribution), thus ensuring consistency with the results of the Gaussian process.</p><h3 id=7-prediction>7. Prediction</h3><p>Assume
$Y_1$ is the sample value,
$Y_2$ is the predicted value and
$m(x)=0$. By the definition of Gaussian Process,
$$\Rightarrow P(Y_1(X),Y_2(X))\sim N(\vec{0},\left(\begin{matrix}K(X_{Y_1},X_{Y_1})+\sigma_y^2I_n&K(X_{Y_1},X_{Y_2})\\K(X_{Y_2},X_{Y_1})&K(X_{Y_2},X_{Y_2})\end{matrix}\right))$$
$$\Rightarrow P(Y_2|Y_1) = \frac{P(Y_1,Y_2)}{P(Y_1)}$$
$$\Rightarrow P(Y_2|Y_1) \sim N(K(X_{Y_2},X_{Y_1})(K(X_{Y_1},X_{Y_1})+\sigma^2I_n)^{-1}Y_2,K(X_{Y_2},X_{Y_2})-K(X_{Y_2},X_{Y_1})K(X_{Y_1},X_{Y_1})^{-1}K(X_{Y_1},X_{Y_2}))$$<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_7c139aad98998130b0e13d7357814614.webp 400w,
/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_c8391795b87ddd0adc204f10431f8ae8.webp 760w,
/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_7c139aad98998130b0e13d7357814614.webp width=228 height=174 loading=lazy data-zoomable></div></div></figure></p><p>Till now, we can calculate the conditional probability distribution to predict
$Y_2$ given the sample
$Y_1$. Looking at these two parameters separately,</p><ul><li>the covariance of the conditional probability:
$\Sigma_{Y_2|Y_1}=K(X_{Y_2},X_{Y_2})-K(X_{Y_2},X_{Y_1})K(X_{Y_1},X_{Y_1})^{-1}K(X_{Y_1},X_{Y_2})\Leftrightarrow$ predictive uncertainty = a priori uncertainty - reduced uncertainty after obtaining samples.
$K(X_{Y_2},X_{Y_2})$ is derived from the covaraince function
$K(x_i,x_j)$ given in the previous definition of the prior, computed from the given inputs, so it can be seen as uncertainty in the prior.
$K(X_{Y_1},X_{Y_2})$ is obtained from the inputs corresponding to the predicted values
$X_{Y_2}$, with the inputs corresponding to the samples
$X_{Y_1}$ calculated by the covariance function, which quantifies the correlation between the samples and the predicted inputs.
$K(X_{Y_1},X_{Y_1})$ is the covariance matrix obtained from the sample. If the whole second term of the formula can be seen as the exponential term in the Gaussian distribution, and simply
$K(X_{Y_2},X_{Y_1})$ as
$(X_{Y_2}-X_{Y_1})$ (both measure the relationship between points), then it can be interpreted as follows, the higher the correlation with the sample (closer to the mean), the larger the value
$K(X_{Y_2},X_{Y_1})$ will be. Note the negative sign in the covariance,
$\Rightarrow$ Reduce more uncertainty
$\Rightarrow$ Higher correlation with the sample
$\Rightarrow$ Lower uncertainty of the predicition.</li></ul><p>The mean of marginal distribution:
$$
\mu_{Y_2|Y_1}=K(X_{Y_2},X_{Y_1})(K(X_{Y_1},X_{Y_1})+\sigma_y^2I_n)^{-1}Y_2\\ =\sum_{i=1}^{n}\alpha_ik(x^{Y_1}_i,x_{pred})\\ (\vec{\alpha}= K(X_{Y_1},X_{Y_1})+\sigma^2I_n)^{-1}Y_2)
$$
It is worth noting that the mean of the entire predictive distribution can be seen as a weighted average of the sample outputs Y1 (in general, the more relevant the sample, the larger the corresponding weight). Also this form coincides with the dual form of L2 regression.</p><h3 id=8-weight-space>8. Weight Space</h3><p>The above has discussed Gaussian regression from the view of function space, and weight space is another perspective to understand.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_187ead7188237b7e5859f55b47cfbcb8.webp 400w,
/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_25a6fce1a6fb0abd787b4921a3e49ae2.webp 760w,
/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_187ead7188237b7e5859f55b47cfbcb8.webp width=505 height=315 loading=lazy data-zoomable></div></div></figure>The above figure depicts linear regression from the weight space, which is restricted to a specific functional form with a prior distribution (
$\mathcal{N}(0,C)$) added to the weights, and the likelihood of the whole form can be written as<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_bf541e315289d0e7446c3857a8b0a721.webp 400w,
/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_9af037b5cbaff6333035c5b823543383.webp 760w,
/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_bf541e315289d0e7446c3857a8b0a721.webp width=647 height=138 loading=lazy data-zoomable></div></div></figure>The posterior distribution of the weights can be obtained by integrating the prior distribution of the likelihood and the weights through the Bayesian formula<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_5457a724f0ffe34074582d561d300975.webp 400w,
/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_4b8e597238208c47317fc2901808229e.webp 760w,
/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_5457a724f0ffe34074582d561d300975.webp width=720 height=263 loading=lazy data-zoomable></div></div></figure>For a Gaussian distribution, mode = mean, so we obtain the maximum a posteriori estimation
$=\bar{w}$. Looking at the mean, considering the standard noise case
$\sigma_n^2=1$, the prior covariance of
$w$ is
$\lambda I_n$,which gives the least squares estimate of L2 regression
$\bar{w}=(XX^T+\lambda I_n)^{-1}Xy$. (In fact, when
$y$ satisfies the assumption of a Gaussian distribution, the least squares estimate of the parameter
$w$ is equivalent to the maximum likelihood estimate). The L2 regression can be viewed in a Bayesian way as adding a prior distribution of
$\mathcal{N}(0,\lambda I_n)$ to the parameter
$w$ (The L1 regression can be seen as adding a
$Laplace(0,\frac{1}{\lambda})$ prior to the parameter
$w$). Furthermore, we can think of a general linear regression as having an uniform prior distribution (improper prior) for parameter
$w$). Finally, returning to the problem of infinitely possible functions, where the weight space restricts the form/type of the function, it is necessary to construct feature maps explicitly to transform different functional forms. This again aligns the idea of implicit construction of features using the kernel in function space.</p><ul><li>Reference:<ul><li>[1] C.E.Rasmussen&amp;C.K.I Williams, Gaussian Process for Machine Learning (GPML), 2006</li><li>[2] CM.Bishop, Pattern Recognition and Machine Learning (PRML), 2006</li><li>[3] Wilson, Andrew, and Ryan Adams. Gaussian process kernels for pattern discovery and extrapolation, ICML,2013</li></ul></li></ul><!-- ### [â¤ï¸ Click here to become a sponsor and help support Wowchemy's future â¤ï¸](https://wowchemy.com/sponsor/)

As a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/sponsor/) awesome rewards and extra features ðŸ¦„âœ¨**

## Ecosystem

- **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX

## Inspiration

[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.

## Features

- **Page builder** - Create _anything_ with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.

## Themes

Wowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.

[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.

## License

Copyright 2016-present [George Cushen](https://georgecushen.com).

Released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md) license. --></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F&amp;text=From+gaussian+process+back+to+linear+regression" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F&amp;t=From+gaussian+process+back+to+linear+regression" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=From%20gaussian%20process%20back%20to%20linear%20regression&amp;body=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F&amp;title=From+gaussian+process+back+to+linear+regression" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=From+gaussian+process+back+to+linear+regression%20https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpr%2F&amp;title=From+gaussian+process+back+to+linear+regression" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://yyimingucl.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu7ed02a851e3119f9241d7a86e4773d4c_43299_270x270_fill_q75_lanczos_center.jpg alt="Yiming Yang"></a><div class=media-body><h5 class=card-title><a href=https://yyimingucl.github.io>Yiming Yang</a></h5><h6 class=card-subtitle>PhD Student in Statistical Science and Advanced Computing</h6><ul class=network-icon aria-hidden=true><li><a href=/><i class="fas fa-"></i></a></li><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/yyimingucl target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/yiming-yang-b87b72193/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>