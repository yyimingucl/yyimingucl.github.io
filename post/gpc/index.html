<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: March 18, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Yiming Yang"><meta name=description content="Introduction In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as &lsquo;finding a function mapping from input $x$ to output $y$&rsquo;."><link rel=alternate hreflang=en-us href=https://yyimingucl.github.io/post/gpc/><link rel=canonical href=https://yyimingucl.github.io/post/gpc/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://yyimingucl.github.io/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Yiming Yang"><meta property="og:url" content="https://yyimingucl.github.io/post/gpc/"><meta property="og:title" content="Gaussian process classification and its approximate inference approaches | Yiming Yang"><meta property="og:description" content="Introduction In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as &lsquo;finding a function mapping from input $x$ to output $y$&rsquo;."><meta property="og:image" content="https://yyimingucl.github.io/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-12-13T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-13T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://yyimingucl.github.io/post/gpc/"},"headline":"Gaussian process classification and its approximate inference approaches","datePublished":"2022-12-13T00:00:00Z","dateModified":"2022-12-13T00:00:00Z","author":{"@type":"Person","name":"Yiming Yang"},"publisher":{"@type":"Organization","name":"Yiming Yang","logo":{"@type":"ImageObject","url":"https://yyimingucl.github.io/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_192x192_fill_lanczos_center_3.png"}},"description":"Introduction In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as \u0026lsquo;finding a function mapping from input $x$ to output $y$\u0026rsquo;."}</script><title>Gaussian process classification and its approximate inference approaches | Yiming Yang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=9798cdfe7888cc4356cef40a362f048e><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Yiming Yang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Yiming Yang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#four-legged_partners><span>Four-legged families</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Gaussian process classification and its approximate inference approaches</h1><div class=article-metadata><div><span class=author-highlighted>Yiming Yang</span></div><span class=article-date>Dec 13, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>5 min read</span></div></div><div class=article-container><div class=article-style><h2 id=introduction>Introduction</h2><p>In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as &lsquo;finding a function mapping from input
$x$ to output
$y$&rsquo;. However, compared to regression problems, Gaussian processes encounter many challenging issues when dealing with classification problems. This article will elucidate these issues and provide corresponding solutions.</p><h2 id=1-review-gaussian-process-regression>1. Review: Gaussian Process Regression</h2><p>A Gaussian process is a stochastic process where any point
$x\in R^d$ is assigned a random variable
$f$ and the joint distribution of these variables follows a Gaussian distribution
$f|x\sim/mathcal{N}(\mu,K)$. Gaussian process is a prior over functions, whose shape (smoothness, etc.) is defined by the mean function
$\mu$ and the covariance
$K=k(X,X)$ where k is a parameterized kernel function. For
$\mu$, we generally set it to 0 (ie.
$\mu(\,\cdot\,)=0$). Given a set of input values
$X$ and their corresponding noisy observations
$y$, we want to predict the function value
$f^*$ at the new point
$x^*$. The joint distribution of the observed values
$y$ and the predicted value
$f^*$ is a Gaussian distribution, which has the following form:
$$
y,f^*|X,x^*\sim/mathcal{N}(\begin{bmatrix} y\\ f^*\\ \end{bmatrix}|\,0,\begin{bmatrix} K_y&k_*\\            k_*&k_{**}\\     \end{bmatrix})\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(\text{Recap 1.})
$$
$K_y=K+\sigma_y^2I$,
$k_*=/mathcal{k}(X,x_*)$, and
$k_{**}=/mathcal{k}(x^*,\bm{x}^*)$.
$\sigma_y^*$ is used to model the noise in the observed values. By applying Bayes&rsquo; theorem to the joint distribution above, we obtain the predictive distribution for
$f^*$:
$$
f^*|x_*,X,y\sim/mathcal{N}(f^*|\mu_*,\Sigma_*),\,\,\,\,\mu_*=k_*^TK_y^{-1}y\,\,,\,\Sigma_*=k_{**}-k_*^TK_y^{-1}k_*\,\,\,\,\,\,\,\,\,\,\,\,(\text{Recap 2.})
$$
You can check the <a href=https://yyimingucl.github.io/post/gpr/ target=_blank rel=noopener>previous article</a> for more information.</p><h2 id=2-binary-classifcation-problem>2. Binary Classifcation Problem</h2><p>In classification problems, our target variable
$y$ is no longer continuous, but rather discrete:
$y = \{+1, -1\}$. Clearly, we can no longer assume that
$y|x$ follows a Gaussian distribution as in regression problems. A suitable choice is the Bernoulli distribution
$y|x \sim Bernoulli(\theta): p(y=+1|x) = \theta, p(y=-1|x) = 1-\theta$, where
$\theta \in [0,1]$. In fact, once we have a good estimate for
$\theta$, the classification problem is solved (from a discriminative point of view. Another way of looking at classification problems, called the generative perspective, aims to estimate the joint distribution of
$y$ and
$x$, which is not discussed here). But how can we estimate
$\theta$? There are many methods for estimating
$\theta$, such as linear models (logistic regression), neural networks (convolutional neural networks in image recognition), etc. Our title is Gaussian process classification, so naturally we will discuss how to use Gaussian processes to estimate
$\theta$.</p><p>Can we directly treat
$\theta$ as a regression variable (
$y$) and use a Gaussian process to obtain
$\theta|x ~ /mathcal{N}(\mu, K)$. The answer is obviously no. There are two reasons for this: (1) the range of
$\theta$ obtained from the Gaussian process is
$(-\infty,\infty)$, which does not satisfy the requirement that
$\theta\in[0,1]$, and (2) although we want to estimate
$\theta$, we cannot directly observe
$\theta$. We can only observe
$y$ produced by
$\theta$. To address the first issue, we can use a response function
$\sigma(\,\cdot\,)$ to compress the results obtained from the Gaussian process into the
$[0,1]$ range. Common response functions include the logistic function and the cumulative probability function of the standard Gaussian distribution (probit function). The figure below shows these two functions, as well as the compressed Gaussian process prior:<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=png srcset="/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_c7a49d25b51e19829c9eeb55e8d4739c.webp 400w,
/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_b2f1f2a03facf356e809e721953d69b4.webp 760w,
/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_c7a49d25b51e19829c9eeb55e8d4739c.webp width=760 height=374 loading=lazy data-zoomable></div></div></figure></p><h2 id=3-gaussian-process-classification-gpc>3. Gaussian Process Classification (GPC)</h2><p>For distinctions, we denote the regression variable of the Gaussian process as the (latent) variable
$f: f|x \sim /mathcal{N}(\mu, K)$. With the response function
$\sigma$ introduced in the previous paragraph, we can obtain the likelihood function. For a sample
$(xi, yi)$, their likelihood is given by:
$$
\begin{equation} p(y_i|x_i,f_i)=\left\{ \begin{array}{ll} \sigma(f_i(x_i)), & y_i=+1 \\       1-\sigma(f_i(x_i)), & y_i=-1 \\ \end{array}  \right.  \end{equation}
$$
Due to the symmetry of the response function:
$\sigma(-z)=1-\sigma(z)$, the likelihood can be expressed more concisely as
$p(y_i|x_i,f_i)=\sigma(y_if_i(x_i))$. It&rsquo;s interesting that for the latent variable f, we don&rsquo;t observe its value (only observe input
${x_i}_i$ and target values
${y_i}_i$) and we are not interested in it at all. The existence of
$f$ is only for the convenience of modeling discrete y and making the model structure clearer. What we are really interested in is
$\pi(x) = p(y=1|x)$, especially for new input
$x^*$, and note that
$\pi(x)$ no longer depends on
$f$. So <strong>how do we remove this dependence?</strong></p><p>Given the sample
$\{X,y\}$, the prediction distribution for a new input
$x^*$ can be expressed as:
$$
\pi(x^*)=p(y^*=1|X,y,x^*)=\int\sigma(f^*)\underbrace{p(f^*|X,y,x^*)}_{The predictive distributio no latent variable f^* }\,df^*\,\,\,\,\,\,\,\,(1)
$$
$$
p(f^*|X,y,x^*)=\int p(f^*|X,x^*,f)p(f|X,y)\,df\,\,\,\,\,\,\,\,\,\,\,\,(2)
$$
$p(f|X,y)=\frac{p(y|f)p(f|X)}{p(y|X)}$ is the posterior distribution of latent variable
$f$. If we want to solve (1), there are two tricky problems: 1. The posterior distribution of the latent variable
$f, p(f|X,y)$, is no longer a Gaussian distribution (where
$p(y|f) = \sigma(yf)$ is a non-Gaussian likelihood). 2. The non-linear function
$\sigma$ applied to
$f^*, \sigma(f^*)$. These two issues make the integration in (1) no longer have a closed-form solution like in regression problems. Approximation is inevitable in this case. This leads us to our second question: <strong>how to approximate the integration in (1)?</strong> Two commonly used methods are given below: 1. Laplacian approximation and 2. Expectation propagation.</p><h2 id=4-laplacian-approximation>4. Laplacian approximation</h2><h3 id=41-introduction>4.1 Introduction</h3><p>The idea of Laplacian approximation is simple: approximate an unknown distribution
$p$ using a Gaussian distribution
$q$. The question is, <strong>how do we determine the parameters
$\mu$ and
$\Sigma$ of the Gaussian distribution
$q$?</strong> Let&rsquo;s start by introducing Laplace&rsquo;s method briefly. Suppose we know that a function
$g(x)$ attains its maximum at
$x_0$, and we want to evaluate the integral
$\int_a^b g(x)dx$.
$$
\begin{split} &\text{Firstly, we define $h(x)=\log(g(x))$}\\ &\Rightarrow \int_a^bg(x)\,dx = \int_a^b\exp(h(x))\,dx\\ &\text{Take Second order Taylor expansion of $h(x)$ at $x_0$}\\ &\Rightarrow\int_a^b \exp(h(x_0)+h'(x_0)(x-x_0)+\frac{1}{2}h''(x_0)(x-x_0)^2)\,dx\\ &\text{we know $g(x)$ will be maximum at $x_0$ $\Rightarrow h(x)$ will also be maximum at $x_0$ $\Rightarrow h'(x_0)=0$}\\ &\Rightarrow\int_a^bg(x)\,dx\approx \exp(h(x_0))\sqrt{2\pi h''(x_0)}\int_a^b\underbrace{\frac{1}{\sqrt{2\pi h''(x_0)}}\exp(\frac{1}{2}h''(x_0)(x-x_0)^2)}_{\mathcal{N}(x_0,h''(x_0))}\,dx\\ &\Rightarrow \text{we only need to find $x_0$ and compute $h''(x_0)$, then we can get the approximate of desired integral} \end{split}
$$</p><h3 id=42-posterior-distribution-hahahugoshortcodes77hbhb>4.2 Posterior distribution
$p(f|X,y)$</h3><!-- ### [❤️ Click here to become a sponsor and help support Wowchemy's future ❤️](https://wowchemy.com/sponsor/)

As a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/sponsor/) awesome rewards and extra features 🦄✨**

## Ecosystem

- **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX

## Inspiration

[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.

## Features

- **Page builder** - Create _anything_ with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 34+ language packs including English, 中文, and Português
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.

## Themes

Wowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.

[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.

## License

Copyright 2016-present [George Cushen](https://georgecushen.com).

Released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md) license. --></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F&amp;text=Gaussian+process+classification+and+its+approximate+inference+approaches" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F&amp;t=Gaussian+process+classification+and+its+approximate+inference+approaches" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Gaussian%20process%20classification%20and%20its%20approximate%20inference%20approaches&amp;body=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F&amp;title=Gaussian+process+classification+and+its+approximate+inference+approaches" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Gaussian+process+classification+and+its+approximate+inference+approaches%20https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fyyimingucl.github.io%2Fpost%2Fgpc%2F&amp;title=Gaussian+process+classification+and+its+approximate+inference+approaches" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://yyimingucl.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu7ed02a851e3119f9241d7a86e4773d4c_43299_270x270_fill_q75_lanczos_center.jpg alt="Yiming Yang"></a><div class=media-body><h5 class=card-title><a href=https://yyimingucl.github.io>Yiming Yang</a></h5><h6 class=card-subtitle>PhD Student in Statistical Science and Advanced Computing</h6><ul class=network-icon aria-hidden=true><li><a href=/><i class="fas fa-"></i></a></li><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/yyimingucl target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/yiming-yang-b87b72193/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>