<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Yiming Yang</title><link>https://yyimingucl.github.io/</link><atom:link href="https://yyimingucl.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Yiming Yang</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://yyimingucl.github.io/media/icon_hu718bbe824f05a3d2309b511845d9303e_95507_512x512_fill_lanczos_center_3.png</url><title>Yiming Yang</title><link>https://yyimingucl.github.io/</link></image><item><title>Example Talk</title><link>https://yyimingucl.github.io/talk/example-talk/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/talk/example-talk/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">page elements&lt;/a> such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Gaussian process classification and its approximate inference approaches</title><link>https://yyimingucl.github.io/post/gpc/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/post/gpc/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as &amp;lsquo;finding a function mapping from input
$x$ to output
$y$&amp;rsquo;. However, compared to regression problems, Gaussian processes encounter many challenging issues when dealing with classification problems. This article will elucidate these issues and provide corresponding solutions.&lt;/p>
&lt;h2 id="1-review-gaussian-process-regression">1. Review: Gaussian Process Regression&lt;/h2>
&lt;p>A Gaussian process is a stochastic process where any point
$x\in R^d$ is assigned a random variable
$f$ and the joint distribution of these variables follows a Gaussian distribution
$f|x\sim/mathcal{N}(\mu,K)$. Gaussian process is a prior over functions, whose shape (smoothness, etc.) is defined by the mean function
$\mu$ and the covariance
$K=k(X,X)$ where k is a parameterized kernel function. For
$\mu$, we generally set it to 0 (ie.
$\mu(\,\cdot\,)=0$). Given a set of input values
$X$ and their corresponding noisy observations
$y$, we want to predict the function value
$f^*$ at the new point
$x^*$. The joint distribution of the observed values
$y$ and the predicted value
$f^*$ is a Gaussian distribution, which has the following form:
$$
y,f^*|X,x^*\sim/mathcal{N}(\begin{bmatrix} y\\ f^*\\ \end{bmatrix}|\,0,\begin{bmatrix} K_y&amp;k_*\\ k_*&amp;k_{**}\\ \end{bmatrix})\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(\text{Recap 1.})
$$
$K_y=K+\sigma_y^2I$,
$k_*=/mathcal{k}(X,x_*)$, and
$k_{**}=/mathcal{k}(x^*,\bm{x}^*)$.
$\sigma_y^*$ is used to model the noise in the observed values. By applying Bayes&amp;rsquo; theorem to the joint distribution above, we obtain the predictive distribution for
$f^*$:
$$
f^*|x_*,X,y\sim/mathcal{N}(f^*|\mu_*,\Sigma_*),\,\,\,\,\mu_*=k_*^TK_y^{-1}y\,\,,\,\Sigma_*=k_{**}-k_*^TK_y^{-1}k_*\,\,\,\,\,\,\,\,\,\,\,\,(\text{Recap 2.})
$$
You can check the &lt;a href="https://yyimingucl.github.io/post/gpr/" target="_blank" rel="noopener">previous article&lt;/a> for more information.&lt;/p>
&lt;h2 id="2-binary-classifcation-problem">2. Binary Classifcation Problem&lt;/h2>
&lt;p>In classification problems, our target variable
$y$ is no longer continuous, but rather discrete:
$y = \{+1, -1\}$. Clearly, we can no longer assume that
$y|x$ follows a Gaussian distribution as in regression problems. A suitable choice is the Bernoulli distribution
$y|x \sim Bernoulli(\theta): p(y=+1|x) = \theta, p(y=-1|x) = 1-\theta$, where
$\theta \in [0,1]$. In fact, once we have a good estimate for
$\theta$, the classification problem is solved (from a discriminative point of view. Another way of looking at classification problems, called the generative perspective, aims to estimate the joint distribution of
$y$ and
$x$, which is not discussed here). But how can we estimate
$\theta$? There are many methods for estimating
$\theta$, such as linear models (logistic regression), neural networks (convolutional neural networks in image recognition), etc. Our title is Gaussian process classification, so naturally we will discuss how to use Gaussian processes to estimate
$\theta$.&lt;/p>
&lt;p>Can we directly treat
$\theta$ as a regression variable (
$y$) and use a Gaussian process to obtain
$\theta|x ~ /mathcal{N}(\mu, K)$. The answer is obviously no. There are two reasons for this: (1) the range of
$\theta$ obtained from the Gaussian process is
$(-\infty,\infty)$, which does not satisfy the requirement that
$\theta\in[0,1]$, and (2) although we want to estimate
$\theta$, we cannot directly observe
$\theta$. We can only observe
$y$ produced by
$\theta$. To address the first issue, we can use a response function
$\sigma(\,\cdot\,)$ to compress the results obtained from the Gaussian process into the
$[0,1]$ range. Common response functions include the logistic function and the cumulative probability function of the standard Gaussian distribution (probit function). The figure below shows these two functions, as well as the compressed Gaussian process prior:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_c7a49d25b51e19829c9eeb55e8d4739c.webp 400w,
/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_b2f1f2a03facf356e809e721953d69b4.webp 760w,
/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpc/response_function_huae593dbf8c9c4ee1c375518b90fd44a7_116714_c7a49d25b51e19829c9eeb55e8d4739c.webp"
width="760"
height="374"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="3-gaussian-process-classification-gpc">3. Gaussian Process Classification (GPC)&lt;/h2>
&lt;p>For distinctions, we denote the regression variable of the Gaussian process as the (latent) variable
$f: f|x \sim /mathcal{N}(\mu, K)$. With the response function
$\sigma$ introduced in the previous paragraph, we can obtain the likelihood function. For a sample
$(xi, yi)$, their likelihood is given by:
$$
\begin{equation} p(y_i|x_i,f_i)=\left\{ \begin{array}{ll} \sigma(f_i(x_i)), &amp; y_i=+1 \\ 1-\sigma(f_i(x_i)), &amp; y_i=-1 \\ \end{array} \right. \end{equation}
$$
Due to the symmetry of the response function:
$\sigma(-z)=1-\sigma(z)$, the likelihood can be expressed more concisely as
$p(y_i|x_i,f_i)=\sigma(y_if_i(x_i))$. It&amp;rsquo;s interesting that for the latent variable f, we don&amp;rsquo;t observe its value (only observe input
${x_i}_i$ and target values
${y_i}_i$) and we are not interested in it at all. The existence of
$f$ is only for the convenience of modeling discrete y and making the model structure clearer. What we are really interested in is
$\pi(x) = p(y=1|x)$, especially for new input
$x^*$, and note that
$\pi(x)$ no longer depends on
$f$. So &lt;strong>how do we remove this dependence?&lt;/strong>&lt;/p>
&lt;p>Given the sample
$\{X,y\}$, the prediction distribution for a new input
$x^*$ can be expressed as:
$$
\pi(x^*)=p(y^*=1|X,y,x^*)=\int\sigma(f^*)\underbrace{p(f^*|X,y,x^*)}_{The predictive distributio no latent variable f^* }\,df^*\,\,\,\,\,\,\,\,(1)
$$
$$
p(f^*|X,y,x^*)=\int p(f^*|X,x^*,f)p(f|X,y)\,df\,\,\,\,\,\,\,\,\,\,\,\,(2)
$$
$p(f|X,y)=\frac{p(y|f)p(f|X)}{p(y|X)}$ is the posterior distribution of latent variable
$f$. If we want to solve (1), there are two tricky problems: 1. The posterior distribution of the latent variable
$f, p(f|X,y)$, is no longer a Gaussian distribution (where
$p(y|f) = \sigma(yf)$ is a non-Gaussian likelihood). 2. The non-linear function
$\sigma$ applied to
$f^*, \sigma(f^*)$. These two issues make the integration in (1) no longer have a closed-form solution like in regression problems. Approximation is inevitable in this case. This leads us to our second question: &lt;strong>how to approximate the integration in (1)?&lt;/strong> Two commonly used methods are given below: 1. Laplacian approximation and 2. Expectation propagation.&lt;/p>
&lt;h2 id="4-laplacian-approximation">4. Laplacian approximation&lt;/h2>
&lt;h3 id="41-introduction">4.1 Introduction&lt;/h3>
&lt;p>The idea of Laplacian approximation is simple: approximate an unknown distribution
$p$ using a Gaussian distribution
$q$. The question is, &lt;strong>how do we determine the parameters
$\mu$ and
$\Sigma$ of the Gaussian distribution
$q$?&lt;/strong> Let&amp;rsquo;s start by introducing Laplace&amp;rsquo;s method briefly. Suppose we know that a function
$g(x)$ attains its maximum at
$x_0$, and we want to evaluate the integral
$\int_a^b g(x)dx$.
$$
\begin{split} &amp;\text{Firstly, we define $h(x)=\log(g(x))$}\\ &amp;\Rightarrow \int_a^bg(x)\,dx = \int_a^b\exp(h(x))\,dx\\ &amp;\text{åœ¨ $x_0$ å¯¹ $h(x)$ Take Second order Taylor expansion}\\ &amp;\Rightarrow\int_a^b \exp(h(x_0)+h'(x_0)(x-x_0)+\frac{1}{2}h''(x_0)(x-x_0)^2)\,dx\\ &amp;\text{we know $g(x)$ will be maximum at $x_0$ $\Rightarrow h(x)$ will also be maximum at $x_0$ $\Rightarrow h'(x_0)=0$}\\ &amp;\Rightarrow\int_a^bg(x)\,dx\approx \exp(h(x_0))\sqrt{2\pi h''(x_0)}\int_a^b\underbrace{\frac{1}{\sqrt{2\pi h''(x_0)}}\exp(\frac{1}{2}h''(x_0)(x-x_0)^2)}_{\mathcal{N}(x_0,h''(x_0))}\,dx\\ &amp;\Rightarrow \text{we only need to find $x_0$ and compute $h''(x_0)$, then we can get the approximate of desired integral} \end{split}
$$
&lt;/p>
&lt;!-- ### [â¤ï¸ Click here to become a sponsor and help support Wowchemy's future â¤ï¸](https://wowchemy.com/sponsor/)
As a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/sponsor/) awesome rewards and extra features ðŸ¦„âœ¨**
## Ecosystem
- **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX
## Inspiration
[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.
## Features
- **Page builder** - Create _anything_ with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.
## Themes
Wowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.
[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.
## License
Copyright 2016-present [George Cushen](https://georgecushen.com).
Released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md) license. --></description></item><item><title>From gaussian process back to linear regression</title><link>https://yyimingucl.github.io/post/gpr/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/post/gpr/</guid><description>&lt;h1 id="overview">Overview&lt;/h1>
&lt;p>In regression problem, we are actually looking for a function that maps input
$x$ to output
$y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,&amp;hellip;) and optimize their weights to approximate the true function. The problem with this approach is that we need to decide what kind of functions to use, and if the chosen type of function does not naturally match the underlying function, we can never obatin a well-apprxoimated function by only adjusting the weights. 2. The second considers arbitrary functions and chooses the one that fits the given sample
$(X, Y)$ more closely (greater likelihood). The problem with this method is that it needs to consider an infinite number of functions, but this is not possible and thus requires the use of the Gaussian process in the title.&lt;/p>
&lt;p>Firstly, I will give the definition of &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process" target="_blank" rel="noopener">Gaussian Process&lt;/a> from wikipedia: A Gaussian process is a stochastic process (a collection of random variables indexed by time or space) such that every finite collection of those random variables forms a multivariate normal distribution. Remark: &lt;strong>Every finite collection of those random variables forms a multivariate normal distribution&lt;/strong>. This property will be our panacea for any upcoming troubles!
correlation&lt;/p>
&lt;h3 id="1-two-dimensional-gaussian-distribution">1. Two-dimensional Gaussian distribution&lt;/h3>
&lt;p>From the above definition, it is clear that any number of random variables form a Gaussian distribution. To simplify the problem, we firstly take two of these random variables such that
$(x_1,x_2)\sim\mathcal{N}(\mu,\Sigma)$. Three pictures below: (left) Density contours of the distribution. (right) Sampled values of the random variables
$x_1,x_2$.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_3747dd93f11ee0bd224a6f8f8f786882.webp 400w,
/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_e479320415b21d1605f06ad243e31cb6.webp 760w,
/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/2d_gaussian_cov0_huea498599102b864f1c3e8ae5de1c4a5d_47483_3747dd93f11ee0bd224a6f8f8f786882.webp"
width="608"
height="353"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>cov(x1,x2)=0&lt;/em>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_e73a2b196d631caf75fc76a53b058ca8.webp 400w,
/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_cf0c856d4472984b4a43237d0404f514.webp 760w,
/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/2d_gaussian_cov0.7_huc876f75d4645b4db463d9003e45a3853_45908_e73a2b196d631caf75fc76a53b058ca8.webp"
width="608"
height="353"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>cov(x1,x2)=0.7&lt;/em>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c8ad0e440fc78778d04f3144c4f9b0cd.webp 400w,
/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c6fc88e86678dabdb3d3e319caf0fcfd.webp 760w,
/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/2d_gaussian_cov0.95_hub8ece08e22daa649a647f218f2872f8f_38769_c8ad0e440fc78778d04f3144c4f9b0cd.webp"
width="608"
height="353"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>cov(x1,x2)=0.95&lt;/em>
As you can see from the graph above, as the correlation between
$x_1,x_2$ gets larger, the values of
$x_1$ and
$x_2$ that we sample become more and more similar. (As can be expected, when the correlation is close to 1, no matter how many times we sample,
$x_1$ is always equal to
$x_2$.&lt;/p>
&lt;h3 id="2-high-dimensional-gaussian-distribution">2. High-dimensional Gaussian distribution&lt;/h3>
&lt;p>The simple two-dimensional case was studied, let&amp;rsquo;s now extend to 20 dimensions.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_9a21ff4f7208871dbca5533da46ca197.webp 400w,
/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_aed4ff7f6dbca320452b6dadfe614fd3.webp 760w,
/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/finite_sample_demo_hu304a5f27892917a64a09be6e5e281a14_38713_9a21ff4f7208871dbca5533da46ca197.webp"
width="760"
height="247"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>20-dimensional Gaussian distribution&lt;/em>&lt;/p>
&lt;p>The two left figures above, same as in 2D case, are sampled values for
$(x_1,x_2,...x_{20})$ obtained from a 20-dimensional Gaussian distribution (Do the shown curves look like the non-linear regressions?) The right panel shows the covariance matrix, where you can see that variables has a strong correlation with their neighbors (also reflected in the two panes on the left, where the adjacent variables do not vary largely, thus making the whole curve very smooth.)&lt;/p>
&lt;p>What if we fix two random variables and sample again?
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_3764b327f86795c728138c48652c72d2.webp 400w,
/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_54b4732790393aa3287fe5d344e9cb64.webp 760w,
/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/finite_sample_with_fixed_dim_hu8ecd30d95359737e48db0127e80d2a3d_30925_3764b327f86795c728138c48652c72d2.webp"
width="608"
height="307"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
What if we viewed the two given random variables as given samples in the regression problem? The graph above can be seen as generating four curves that exactly fit the sample. Using the idea of averaging, we sample many curves and then take the average as our regression curve, but this is costly. Luckily, recalling the previous definition of a Gaussian process, the 20 random variables follow a twenty-dimensional Gaussian distribution, and their conditional probability distribution
$(x_{3:20}|x_1,x_2\sim Gaussian)$ remains a Gaussian distribution according to the properties of the Gaussian distribution.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_b1339589bc5fcd05a732794381e78755.webp 400w,
/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_3810a2271173a8af29ba23f96ff14088.webp 760w,
/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/error_bar_hu90e3d8dc78c5f82f3285083678798f3e_6255_b1339589bc5fcd05a732794381e78755.webp"
width="370"
height="248"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>Variance of each random variable (since the first two variables are already given, they can be seen as constants with a variance of zero. In the mean time, the adjacent variabes have a small variance due to the high correlation with the two variables.)&lt;/em>&lt;/p>
&lt;p>&lt;strong>New Question: The above procedure only discusses the discrete case (integer index dimension), but the actual regression problems are often the continuous case.&lt;/strong> It is a straightforward idea to sample many times at infinite points (1000 dimensions, 10000 dimensions, &amp;hellip;, and infinite dimensions) around the given sample to approximate a continuous function. However, this approach is extremely ineffective and impossible in fact.&lt;/p>
&lt;h3 id="3-gaussian-process-regression">3. Gaussian Process Regression&lt;/h3>
&lt;p>Recall from the previous definition of a Gaussian process that &lt;strong>any number of random variables constitutes a Gaussian distribution&lt;/strong>. Generally speaking, if we take an infinite number of random variables will form an infinite-dimensional Gaussian distribution (infinite-dimensional vector of means, infinite-dimensional * infinite-dimensional covariance matrix). And further, if we consider each function as a very very long vector (an infinite-dimensional vector), then the two parameters of the infinite-dimensional Gaussian distribution, the mean and the variance, can be represented by two functions. The entire Gaussian process can then be written in the form:
$f(\cdot)\sim\mathcal{N}(m(\cdot), K(\cdot,\cdot))$ where
$m(\cdot)\,\,,K(\cdot,\cdot)$ are called the mean function and covariance function respectively. By definition in this way, we can get ride of the limitation of the discrete case and the mean and covariance matrix can be calculated for any function
$f(x)$.&lt;/p>
&lt;p>From this form, it is possible to view the whole Gaussian process as sampling from a Gaussian distribution defined over functions (functional). Like the finite-dimensional Gaussian distribution, it is uniquely determined by the mean and covariance. Recalling the second approach to solving regression mentioned in the beginning, the Gaussian process does take into account all possible functions. Sampling only from this above form has a very low probability of sampling functions that match the sample points. Therefore, if we wish to obtain functions that match the sample, we need to combine the Gaussian process with given samples. From a Bayesian perspective, it is then possible to think of the Gaussian process as a prior distribution over functions. After combining with the given samples, we get the posterior distribution over functions.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_4cb7dc7eedcd74263e1925838a09ca47.webp 400w,
/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_1c1aea4b4a43d3d43ec8b71d775436af.webp 760w,
/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/gaussian_prior_hud7777d377f3db6a0568c29ec18840e6e_30459_4cb7dc7eedcd74263e1925838a09ca47.webp"
width="370"
height="264"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>m( . )=0 and K( . , .) is gaussian kernel with alpha=2,beta=0.1&lt;/em>&lt;/p>
&lt;p>The figure above shows the 5 functions sampled from the prior distribution of this function. The blue region is
$\mu\pm\sigma^2$.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_df1a773d22f2e5ffa1212036f783908f.webp 400w,
/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_cc9c723220ba9373755f149f00973496.webp 760w,
/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/gaussian_posterior_huf485d44c49a3bc36b3f14fa3e0e692ec_30754_df1a773d22f2e5ffa1212036f783908f.webp"
width="370"
height="248"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
The above figure demonstrates the functions sampled in the posterior distribution, given three samples
$(x_1,y_1),(x_2,y_2),(x_3,y_3)$. The thick black line is the mean. It can be seen that, same as in the second part of the 20-dimensional discrete case, the variance around the sample points is almost zero. (Another way to understand the process of combing samples is to reject the functions that do not match these sample points.)&lt;/p>
&lt;h3 id="4-covariance-function-kernel">4. Covariance Function (Kernel)&lt;/h3>
&lt;p>For given sample
$X=((x_1),(x_2),...,(x_n)$ (each column is the feature for each sample
$(x_i)$, then the posterior distribution can be written in the form
$f(X)\sim\mathcal{N}(m(X),K(X,X)$ where
$[K(X,X)]_{ij}=K(x_i,x_j)$.&lt;/p>
&lt;p>Simply starting from the idea of the kernel trick, it is equivalent to quantifies the relationship between points on the feature space induced by the covariance function (though of course the choice of covariance function varies for different situations). Returning to the perspective of the infinite dimensional Gaussian distribution, the covariance function quantifies the relationship between infinitely closed points. In some sense, the covariance function
$K(\cdot\,\,,\cdot)$ determines the overall shape of the function (from an a priori perspective, the covaraince function expresses the priori knowledge of the desired function).&lt;/p>
&lt;p>The covariance function and the choice of its hyperparameters is a really big topic (all the images above use the squared-exponential covariance function
$K(x_i,x_j)=\alpha\exp{-\frac{1}{2l^2}( x_i-x_j)^2}$). The squared-exponential covariance function is used here as an example for a brief discussion.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_e48cc6091b698651e8fda2ee7aeada97.webp 400w,
/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_23462f43502174721c04e8a2075ffe89.webp 760w,
/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/kernel_demo_hueaf4a594b5b1a8d89aecc31c154ae84e_55936_e48cc6091b698651e8fda2ee7aeada97.webp"
width="760"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
The above figure shows when the hyperparameter L in the kernel is small, the correlation between points is smale , and the neighbored points are limited to vary in a small range. (when using this kernel as a priori covariance function, we can imagine that the sampled function will exhibit very large fluctuations). Conversely, when the hyperparameter L is large, one specific point is still correlated with very distant points (The sampled function will be very smooth), and the following figure verifies these descriptions.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_e3fa0fe4b55a62493efd227452f7898d.webp 400w,
/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_078d5070fca6a837e843d92106d91ec1.webp 760w,
/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/covariance_prior_demo_hu6f2e013cd432de7cf5ef3d9b48c51574_97954_e3fa0fe4b55a62493efd227452f7898d.webp"
width="760"
height="471"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>Sample from Gaussian Process by squared-exponential covariance function with different parameter L&lt;/em>&lt;/p>
&lt;h3 id="5-use-gp-for-regression">5. Use GP for regression&lt;/h3>
&lt;p>Summarizing the above process, a prior is first given for all potential functions by Gaussian process, then, combined with the samples, we obtain the posterior distribution for the function
$f(X)\sim\mathcal{N}(m(X),K(X,X)$ where
$[K(X,X)]_{ij}=K(x_i,x_j)$.&lt;/p>
&lt;p>Taken the noise into account, the output function becomes
$y(X)=f(X)+\sigma^2_yI\,\,,\sigma^2_yI\sim\mathcal{N}(0,1)$ and
$y(X)\sim\mathcal{N}(m(X),K(X,X)+\sigma^2_y)$.&lt;/p>
&lt;p>&lt;strong>New Question Again: How can we store an infinite dimensional Gaussian distribution in a computer with finite memory? Similarly, for the posterior distribution given samples
$(X,Y)$, we can see that the parameters are only finite dimensional (the mean is a vector of length n and the covariance matrix is n*n), so does this result contradict or behave inconsistently with the infinite dimensionality?&lt;/strong>&lt;/p>
&lt;h3 id="6-consistency-and-marginalisation-property">6. Consistency and Marginalisation Property&lt;/h3>
&lt;p>Both of these problems can be solved perfectly by the &lt;strong>Marginalisation Property&lt;/strong> of the Gaussian distribution. For Multivariate Gaussian distribution:
$$P(Y_1) = \int_{Y_2}P(Y_1,Y_2)dY_2\\ P(Y_1,Y_2) \sim N( \left(\begin{matrix} a\\ b\end{matrix}\right),\left(\begin{matrix} A &amp;B\\ B^T&amp;C \end{matrix}\right))\Rightarrow P(Y_1)\sim N(a, A)$$
With this property means that we can split only the part of our interest (samples and predictions) from the entire infinite dimensional Gaussian distribution and ignore these parts are not in our interest (no need to interpolation/extrapolation). This property is then used to derive for the distribution of our part of interest. 1. For the first problem, we just need to keep the multivariate Gaussian distribution of the sample part in the computer. 2. For the second problem, it can be seen from the above properties that the parameters of the marginal distribution obtained by partitioning the whole Gaussian distribution are also part of the parameters of the whole Gaussian distribution (or saying derived from the parameters of the infinite-dimensional distribution), thus ensuring consistency with the results of the Gaussian process.&lt;/p>
&lt;h3 id="7-prediction">7. Prediction&lt;/h3>
&lt;p>Assume
$Y_1$ is the sample value,
$Y_2$ is the predicted value and
$m(x)=0$. By the definition of Gaussian Process,
$$\Rightarrow P(Y_1(X),Y_2(X))\sim N(\vec{0},\left(\begin{matrix}K(X_{Y_1},X_{Y_1})+\sigma_y^2I_n&amp;K(X_{Y_1},X_{Y_2})\\K(X_{Y_2},X_{Y_1})&amp;K(X_{Y_2},X_{Y_2})\end{matrix}\right))$$
$$\Rightarrow P(Y_2|Y_1) = \frac{P(Y_1,Y_2)}{P(Y_1)}$$
$$\Rightarrow P(Y_2|Y_1) \sim N(K(X_{Y_2},X_{Y_1})(K(X_{Y_1},X_{Y_1})+\sigma^2I_n)^{-1}Y_2,K(X_{Y_2},X_{Y_2})-K(X_{Y_2},X_{Y_1})K(X_{Y_1},X_{Y_1})^{-1}K(X_{Y_1},X_{Y_2}))$$
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_7c139aad98998130b0e13d7357814614.webp 400w,
/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_c8391795b87ddd0adc204f10431f8ae8.webp 760w,
/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/predictive_distribution_hu8ab9778db2a2e30b50b70e14880517f2_25815_7c139aad98998130b0e13d7357814614.webp"
width="228"
height="174"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Till now, we can calculate the conditional probability distribution to predict
$Y_2$ given the sample
$Y_1$. Looking at these two parameters separately,&lt;/p>
&lt;ul>
&lt;li>the covariance of the conditional probability:
$\Sigma_{Y_2|Y_1}=K(X_{Y_2},X_{Y_2})-K(X_{Y_2},X_{Y_1})K(X_{Y_1},X_{Y_1})^{-1}K(X_{Y_1},X_{Y_2})\Leftrightarrow$ predictive uncertainty = a priori uncertainty - reduced uncertainty after obtaining samples.
$K(X_{Y_2},X_{Y_2})$ is derived from the covaraince function
$K(x_i,x_j)$ given in the previous definition of the prior, computed from the given inputs, so it can be seen as uncertainty in the prior.
$K(X_{Y_1},X_{Y_2})$ is obtained from the inputs corresponding to the predicted values
$X_{Y_2}$, with the inputs corresponding to the samples
$X_{Y_1}$ calculated by the covariance function, which quantifies the correlation between the samples and the predicted inputs.
$K(X_{Y_1},X_{Y_1})$ is the covariance matrix obtained from the sample. If the whole second term of the formula can be seen as the exponential term in the Gaussian distribution, and simply
$K(X_{Y_2},X_{Y_1})$ as
$(X_{Y_2}-X_{Y_1})$ (both measure the relationship between points), then it can be interpreted as follows, the higher the correlation with the sample (closer to the mean), the larger the value
$K(X_{Y_2},X_{Y_1})$ will be. Note the negative sign in the covariance,
$\Rightarrow$ Reduce more uncertainty
$\Rightarrow$ Higher correlation with the sample
$\Rightarrow$ Lower uncertainty of the predicition.&lt;/li>
&lt;/ul>
&lt;p>The mean of marginal distribution:
$$
\mu_{Y_2|Y_1}=K(X_{Y_2},X_{Y_1})(K(X_{Y_1},X_{Y_1})+\sigma_y^2I_n)^{-1}Y_2\\ =\sum_{i=1}^{n}\alpha_ik(x^{Y_1}_i,x_{pred})\\ (\vec{\alpha}= K(X_{Y_1},X_{Y_1})+\sigma^2I_n)^{-1}Y_2)
$$
It is worth noting that the mean of the entire predictive distribution can be seen as a weighted average of the sample outputs Y1 (in general, the more relevant the sample, the larger the corresponding weight). Also this form coincides with the dual form of L2 regression.&lt;/p>
&lt;h3 id="8-weight-space">8. Weight Space&lt;/h3>
&lt;p>The above has discussed Gaussian regression from the view of function space, and weight space is another perspective to understand.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_187ead7188237b7e5859f55b47cfbcb8.webp 400w,
/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_25a6fce1a6fb0abd787b4921a3e49ae2.webp 760w,
/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/bayesian_regression_diagram_hu4960825754a5dd1aeb1427d3c3179a30_28706_187ead7188237b7e5859f55b47cfbcb8.webp"
width="505"
height="315"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
The above figure depicts linear regression from the weight space, which is restricted to a specific functional form with a prior distribution (
$\mathcal{N}(0,C)$) added to the weights, and the likelihood of the whole form can be written as
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_bf541e315289d0e7446c3857a8b0a721.webp 400w,
/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_9af037b5cbaff6333035c5b823543383.webp 760w,
/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/gp_likelihood_hu5ef13cff57228661b6915e952a6e5d38_29803_bf541e315289d0e7446c3857a8b0a721.webp"
width="647"
height="138"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
The posterior distribution of the weights can be obtained by integrating the prior distribution of the likelihood and the weights through the Bayesian formula
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="png" srcset="
/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_5457a724f0ffe34074582d561d300975.webp 400w,
/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_4b8e597238208c47317fc2901808229e.webp 760w,
/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://yyimingucl.github.io/post/gpr/gp_weight_posterior_huf9fe8095cbd6ebafe21888650e13846c_78890_5457a724f0ffe34074582d561d300975.webp"
width="720"
height="263"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
For a Gaussian distribution, mode = mean, so we obtain the maximum a posteriori estimation
$=\bar{w}$. Looking at the mean, considering the standard noise case
$\sigma_n^2=1$, the prior covariance of
$w$ is
$\lambda I_n$,which gives the least squares estimate of L2 regression
$\bar{w}=(XX^T+\lambda I_n)^{-1}Xy$. (In fact, when
$y$ satisfies the assumption of a Gaussian distribution, the least squares estimate of the parameter
$w$ is equivalent to the maximum likelihood estimate). The L2 regression can be viewed in a Bayesian way as adding a prior distribution of
$\mathcal{N}(0,\lambda I_n)$ to the parameter
$w$ (The L1 regression can be seen as adding a
$Laplace(0,\frac{1}{\lambda})$ prior to the parameter
$w$). Furthermore, we can think of a general linear regression as having an uniform prior distribution (improper prior) for parameter
$w$). Finally, returning to the problem of infinitely possible functions, where the weight space restricts the form/type of the function, it is necessary to construct feature maps explicitly to transform different functional forms. This again aligns the idea of implicit construction of features using the kernel in function space.&lt;/p>
&lt;ul>
&lt;li>Reference:
&lt;ul>
&lt;li>[1] C.E.Rasmussen&amp;amp;C.K.I Williams, Gaussian Process for Machine Learning (GPML), 2006&lt;/li>
&lt;li>[2] CM.Bishop, Pattern Recognition and Machine Learning (PRML), 2006&lt;/li>
&lt;li>[3] Wilson, Andrew, and Ryan Adams. Gaussian process kernels for pattern discovery and extrapolation, ICML,2013&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!-- ### [â¤ï¸ Click here to become a sponsor and help support Wowchemy's future â¤ï¸](https://wowchemy.com/sponsor/)
As a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/sponsor/) awesome rewards and extra features ðŸ¦„âœ¨**
## Ecosystem
- **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX
## Inspiration
[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.
## Features
- **Page builder** - Create _anything_ with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.
## Themes
Wowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.
[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.
## License
Copyright 2016-present [George Cushen](https://georgecushen.com).
Released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md) license. --></description></item><item><title>Traffic4cast at NeurIPS 2021 â€“ Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes</title><link>https://yyimingucl.github.io/publication/journal-article/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/publication/journal-article/</guid><description>&lt;!--
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -->
&lt;!-- ---
title: "_Traffic4cast_ at NeurIPS 2021 â€“ Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes"
authors:
- Christian Eichenberger
- Moritz Neun
- Henry Martin
date: "2022-02-12T00:00:00Z"
doi: ""
# Schedule page publish date (NOT publication's date).
publishDate: "2017-01-01T00:00:00Z"
# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["2"]
# Publication name and optional abbreviated publication name.
# publication: "Proceedings of Machine Learning Research"
publication: "*Journal of Source Themes, 1*(1)"
publication_short: "PMLR"
abstract: The IARAI _Traffic4cast_ competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture, demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, _Traffic4cast_ 2021 now focuses on the question of model robustness and generalizability across time and space. Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over 10^12 GPS probe data. Winning solutions captured traffic dynamics
sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input.
# Summary. An optional shortened abstract.
summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.
tags:
- Source Themes
featured: false
# links:
# - name: ""
# url: ""
url_pdf: https://proceedings.mlr.press/v176/eichenberger22a/eichenberger22a.pdf
# url_code: 'https://github.com/wowchemy/wowchemy-hugo-themes'
# url_dataset: ''
# url_poster: ''
# url_project: ''
# url_slides: ''
# url_source: ''
# url_video: ''
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
caption: 'Image credit: [_Traffic4Cast2021_](https://www.iarai.ac.at/traffic4cast/)'
focal_point: ""
preview_only: false
# Associated Projects (optional).
# Associate this publication with one or more of your projects.
# Simply enter your project's folder or file name without extension.
# E.g. `internal-project` references `content/project/internal-project/index.md`.
# Otherwise, set `projects: []`.
projects: []
# Slides (optional).
# Associate this publication with Markdown slides.
# Simply enter your slide deck's filename without extension.
# E.g. `slides: "example"` references `content/slides/example/index.md`.
# Otherwise, set `slides: ""`.
slides: ""
---
&lt;!--
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Dual Encoding U-Net for Spatio-Temporal Domain Shift Frame Prediction</title><link>https://yyimingucl.github.io/publication/preprint/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/publication/preprint/</guid><description>&lt;!--
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -->
&lt;!-- ---
title: "An example preprint / working paper"
authors:
- Jay Santokhi, Yiming Yang, et al.
date: "2021-10-27T00:00:00Z"
doi: ""
# Schedule page publish date (NOT publication's date).
publishDate: "2017-01-01T00:00:00Z"
# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]
# Publication name and optional abbreviated publication name.
publication: ""
publication_short: ""
abstract: The landscape of city-wide mobility behaviour has altered significantly over the
past 18 months. The ability to make accurate and reliable predictions on such
behaviour has likewise changed drastically with COVID-19 measures impacting
how populations across the world interact with the different facets of mobility. This
raises the question - "How does one use an abundance of pre-covid mobility data
to make predictions on future behaviour in a present/post-covid environment?"
This paper seeks to address this question by introducing an approach for traffic
frame prediction using a lightweight Dual-Encoding U-Net built using only 12
Convolutional layers that incorporates a novel approach to skip-connections between Convolutional LSTM layers. This approach combined with an intuitive handling of training data can model both a temporal and spatio-temporal domain
shift.
# Summary. An optional shortened abstract.
summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.
tags:
- Source Themes
featured: false
links:
# - name: Custom Link
# url: http://example.org
url_pdf: https://arxiv.org/pdf/2110.11140.pdf
url_code: 'https://gitlab.com/alchera/alchera-traffic4cast-2021'
url_dataset: 'https://github.com/iarai/NeurIPS2021-traffic4cast'
# url_poster: '#'
# url_project: ''
# url_slides: ''
# url_source: '#'
# url_video: '#'
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
focal_point: ""
preview_only: false
# Associated Projects (optional).
# Associate this publication with one or more of your projects.
# Simply enter your project's folder or file name without extension.
# E.g. `internal-project` references `content/project/internal-project/index.md`.
# Otherwise, set `projects: []`.
projects:
- internal-project
# Slides (optional).
# Associate this publication with Markdown slides.
# Simply enter your slide deck's filename without extension.
# E.g. `slides: "example"` references `content/slides/example/index.md`.
# Otherwise, set `slides: ""`.
slides: example
---
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Example Project</title><link>https://yyimingucl.github.io/project/example/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/project/example/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>External Project</title><link>https://yyimingucl.github.io/project/external-project/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/project/external-project/</guid><description/></item><item><title/><link>https://yyimingucl.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yyimingucl.github.io/admin/config.yml</guid><description/></item></channel></rss>