[{"authors":null,"categories":null,"content":"Hi there, welcome to my personal website. My name is Yiming Yang. I am a first-year PhD student in Statistical Science and Advanced Computing at Department of Statistical Science, University College London (UCL), where I am luckly supervised by Prof. Serge Guillas. Prior to my current role, I completed my Master program MSc Computational Statiscs and Machine Learning from the Department of Computer Science, UCL. I obtained Bachelor’s degree in Mathematics and Statiscs at the Department of Mathematics, UCL. My research is mainly funded by United Kingdom Atomic Energy Authority (UKAEA) and UCL jointly.\nMy research mainly aims at developing new methods to quantify uncertainties in exascale (1 exaFLOP is 10^18 floating point operation per second) simulation models by Statistical Methods and Machine Learning. In partcular, I am working on the Uncertainty Quantification (UQ) of exascale nuclear fusion simulation which is involved in two projects: SEAVEA (Software Environment for Actionable \u0026amp; VVUQ-evaluated Exascale Applications) and AQUIFER (Advanced Quantification of Uncertainties In Fusion modelling at the Exascale with model order Reducation).\nCurrently, I am researching the methodolgies and applications of Linked Gaussian Process (GP) for emulating large-scale coupled physcial simulation. The emulation will siginificately accelerate the exploration of physcial domain which help better study particular physical phenomena and play an important role in the downstream statistical tasks like UQ, Sensitivity Analysis, and so on.\nDownload my CV .\n","date":1670889600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670889600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi there, welcome to my personal website. My name is Yiming Yang. I am a first-year PhD student in Statistical Science and Advanced Computing at Department of Statistical Science, University College London (UCL), where I am luckly supervised by Prof.","tags":null,"title":"Yiming Yang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://yyimingucl.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yiming Yang"],"categories":null,"content":"Introduction In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as ‘finding a function mapping from input $x$ to output $y$’. However, compared to regression problems, Gaussian processes encounter many challenging issues when dealing with classification problems. This article will elucidate these issues and provide corresponding solutions.\n1. Review: Gaussian Process Regression A Gaussian process is a stochastic process where any point $x\\in R^d$ is assigned a random variable $f$ and the joint distribution of these variables follows a Gaussian distribution $f|x\\sim/mathcal{N}(\\mu,K)$. Gaussian process is a prior over functions, whose shape (smoothness, etc.) is defined by the mean function $\\mu$ and the covariance $K=k(X,X)$ where k is a parameterized kernel function. For $\\mu$, we generally set it to 0 (ie. $\\mu(\\,\\cdot\\,)=0$). Given a set of input values $X$ and their corresponding noisy observations $y$, we want to predict the function value $f^*$ at the new point $x^*$. The joint distribution of the observed values $y$ and the predicted value $f^*$ is a Gaussian distribution, which has the following form: $$ y,f^*|X,x^*\\sim/mathcal{N}(\\begin{bmatrix} y\\\\ f^*\\\\ \\end{bmatrix}|\\,0,\\begin{bmatrix} K_y\u0026amp;k_*\\\\ k_*\u0026amp;k_{**}\\\\ \\end{bmatrix})\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(\\text{Recap 1.}) $$ $K_y=K+\\sigma_y^2I$, $k_*=/mathcal{k}(X,x_*)$, and $k_{**}=/mathcal{k}(x^*,\\bm{x}^*)$. $\\sigma_y^*$ is used to model the noise in the observed values. By applying Bayes’ theorem to the joint distribution above, we obtain the predictive distribution for $f^*$: $$ f^*|x_*,X,y\\sim/mathcal{N}(f^*|\\mu_*,\\Sigma_*),\\,\\,\\,\\,\\mu_*=k_*^TK_y^{-1}y\\,\\,,\\,\\Sigma_*=k_{**}-k_*^TK_y^{-1}k_*\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(\\text{Recap 2.}) $$ You can check the previous article for more information.\n2. Binary Classifcation Problem In classification problems, our target variable $y$ is no longer continuous, but rather discrete: $y = \\{+1, -1\\}$. Clearly, we can no longer assume that $y|x$ follows a Gaussian distribution as in regression problems. A suitable choice is the Bernoulli distribution $y|x \\sim Bernoulli(\\theta): p(y=+1|x) = \\theta, p(y=-1|x) = 1-\\theta$, where $\\theta \\in [0,1]$. In fact, once we have a good estimate for $\\theta$, the classification problem is solved (from a discriminative point of view. Another way of looking at classification problems, called the generative perspective, aims to estimate the joint distribution of $y$ and $x$, which is not discussed here). But how can we estimate $\\theta$? There are many methods for estimating $\\theta$, such as linear models (logistic regression), neural networks (convolutional neural networks in image recognition), etc. Our title is Gaussian process classification, so naturally we will discuss how to use Gaussian processes to estimate $\\theta$.\nCan we directly treat $\\theta$ as a regression variable ( $y$) and use a Gaussian process to obtain $\\theta|x ~ /mathcal{N}(\\mu, K)$. The answer is obviously no. There are two reasons for this: (1) the range of $\\theta$ obtained from the Gaussian process is $(-\\infty,\\infty)$, which does not satisfy the requirement that $\\theta\\in[0,1]$, and (2) although we want to estimate $\\theta$, we cannot directly observe $\\theta$. We can only observe $y$ produced by $\\theta$. To address the first issue, we can use a response function $\\sigma(\\,\\cdot\\,)$ to compress the results obtained from the Gaussian process into the $[0,1]$ range. Common response functions include the logistic function and the cumulative probability function of the standard Gaussian distribution (probit function). The figure below shows these two functions, as well as the compressed Gaussian process prior: 3. Gaussian Process Classification (GPC) For distinctions, we denote the regression variable of the Gaussian process as the (latent) variable $f: f|x \\sim /mathcal{N}(\\mu, K)$. With the response function $\\sigma$ introduced in the previous paragraph, we can obtain the likelihood function. For a sample $(xi, yi)$, their likelihood is given by: $$ \\begin{equation} p(y_i|x_i,f_i)=\\left\\{ \\begin{array}{ll} \\sigma(f_i(x_i)), \u0026amp; y_i=+1 \\\\ 1-\\sigma(f_i(x_i)), \u0026amp; y_i=-1 \\\\ \\end{array} \\right. \\end{equation} $$ Due to the symmetry of the response function: $\\sigma(-z)=1-\\sigma(z)$, the likelihood can be expressed more concisely as $p(y_i|x_i,f_i)=\\sigma(y_if_i(x_i))$. It’s interesting that for the latent variable f, we don’t observe its value (only observe input ${x_i}_i$ and target values ${y_i}_i$) and we are not interested in it at all. The existence of $f$ is only for the convenience of modeling discrete y and making the model structure clearer. What we are really interested in is $\\pi(x) = p(y=1|x)$, especially for new input $x^*$, and note that $\\pi(x)$ no longer depends on $f$. So how do we remove this …","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"9798cdfe7888cc4356cef40a362f048e","permalink":"https://yyimingucl.github.io/post/gpc/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/post/gpc/","section":"post","summary":"Introduction In a previous article, we briefly explained the application of Gaussian processes to regression problems. Apart from regression, classification is another important type of problem. Both regression and classification problems can be categorized as ‘finding a function mapping from input $x$ to output $y$’.","tags":null,"title":"Gaussian process classification and its approximate inference approaches","type":"post"},{"authors":["Yiming Yang"],"categories":null,"content":"Overview In regression problem, we are actually looking for a function that maps input $x$ to output $y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,…) and optimize their weights to approximate the true function. The problem with this approach is that we need to decide what kind of functions to use, and if the chosen type of function does not naturally match the underlying function, we can never obatin a well-apprxoimated function by only adjusting the weights. 2. The second considers arbitrary functions and chooses the one that fits the given sample $(X, Y)$ more closely (greater likelihood). The problem with this method is that it needs to consider an infinite number of functions, but this is not possible and thus requires the use of the Gaussian process in the title.\nFirstly, I will give the definition of Gaussian Process from wikipedia: A Gaussian process is a stochastic process (a collection of random variables indexed by time or space) such that every finite collection of those random variables forms a multivariate normal distribution. Remark: Every finite collection of those random variables forms a multivariate normal distribution. This property will be our panacea for any upcoming troubles! correlation\n1. Two-dimensional Gaussian distribution From the above definition, it is clear that any number of random variables form a Gaussian distribution. To simplify the problem, we firstly take two of these random variables such that $(x_1,x_2)\\sim\\mathcal{N}(\\mu,\\Sigma)$. Three pictures below: (left) Density contours of the distribution. (right) Sampled values of the random variables $x_1,x_2$. cov(x1,x2)=0 cov(x1,x2)=0.7 cov(x1,x2)=0.95 As you can see from the graph above, as the correlation between $x_1,x_2$ gets larger, the values of $x_1$ and $x_2$ that we sample become more and more similar. (As can be expected, when the correlation is close to 1, no matter how many times we sample, $x_1$ is always equal to $x_2$.\n2. High-dimensional Gaussian distribution The simple two-dimensional case was studied, let’s now extend to 20 dimensions. 20-dimensional Gaussian distribution\nThe two left figures above, same as in 2D case, are sampled values for $(x_1,x_2,...x_{20})$ obtained from a 20-dimensional Gaussian distribution (Do the shown curves look like the non-linear regressions?) The right panel shows the covariance matrix, where you can see that variables has a strong correlation with their neighbors (also reflected in the two panes on the left, where the adjacent variables do not vary largely, thus making the whole curve very smooth.)\nWhat if we fix two random variables and sample again? What if we viewed the two given random variables as given samples in the regression problem? The graph above can be seen as generating four curves that exactly fit the sample. Using the idea of averaging, we sample many curves and then take the average as our regression curve, but this is costly. Luckily, recalling the previous definition of a Gaussian process, the 20 random variables follow a twenty-dimensional Gaussian distribution, and their conditional probability distribution $(x_{3:20}|x_1,x_2\\sim Gaussian)$ remains a Gaussian distribution according to the properties of the Gaussian distribution. Variance of each random variable (since the first two variables are already given, they can be seen as constants with a variance of zero. In the mean time, the adjacent variabes have a small variance due to the high correlation with the two variables.)\nNew Question: The above procedure only discusses the discrete case (integer index dimension), but the actual regression problems are often the continuous case. It is a straightforward idea to sample many times at infinite points (1000 dimensions, 10000 dimensions, …, and infinite dimensions) around the given sample to approximate a continuous function. However, this approach is extremely ineffective and impossible in fact.\n3. Gaussian Process Regression Recall from the previous definition of a Gaussian process that any number of random variables constitutes a Gaussian distribution. Generally speaking, if we take an infinite number of random variables will form an infinite-dimensional Gaussian distribution (infinite-dimensional vector of means, infinite-dimensional * infinite-dimensional covariance matrix). And further, if we consider each function as a very very long vector (an infinite-dimensional vector), then the two parameters of the infinite-dimensional Gaussian distribution, the mean and the variance, can be represented by two functions. The entire Gaussian process can then be written in the form: $f(\\cdot)\\sim\\mathcal{N}(m(\\cdot), K(\\cdot,\\cdot))$ where $m(\\cdot)\\,\\,,K(\\cdot,\\cdot)$ are called the mean function and covariance function respectively. By definition in this way, we can get ride of the limitation of the discrete case and the mean and covariance matrix can be calculated for any function $f(x)$.\nFrom this form, …","date":1665619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665619200,"objectID":"507055da84bcaa46720eb2b80d881b3e","permalink":"https://yyimingucl.github.io/post/gpr/","publishdate":"2022-10-13T00:00:00Z","relpermalink":"/post/gpr/","section":"post","summary":"Overview In regression problem, we are actually looking for a function that maps input $x$ to output $y$. There are commonly two approaches: 1. We restrict the function space/form (linear, quadratic,…) and optimize their weights to approximate the true function.","tags":null,"title":"From gaussian process back to linear regression","type":"post"},{"authors":["Christian Eichenberger","Moritz Neun","et al."],"categories":null,"content":" ","date":1644624e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644624e3,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://yyimingucl.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"The IARAI _Traffic4cast_ competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, _Traffic4cast_ 2021 now focuses on the question of model robustness and generalizability across time and space Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over 10^12 GPS probe data. Winning solutions captured traffic dynamics sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input.","tags":null,"title":"Traffic4cast at NeurIPS 2021 – Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes","type":"publication"},{"authors":["Jay Santokhi","Yiming Yang","Dylan Hillier","et al."],"categories":null,"content":" ","date":1635292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635292800,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://yyimingucl.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"The landscape of city-wide mobility behaviour has altered significantly over the past 18 months. The ability to make accurate and reliable predictions on such behaviour has likewise changed drastically with COVID-19 measures impacting how populations across the world interact with the different facets of mobility. This raises the question - _\"How does one use an abundance of pre-covid mobility data to make predictions on future behaviour in a present/post-covid environment?\"_ This paper seeks to address this question by introducing an approach for traffic frame prediction using a lightweight Dual-Encoding U-Net built using only 12 Convolutional layers that incorporates a novel approach to skip-connections between Convolutional LSTM layers.This approach combined with an intuitive handling of training data can model both a temporal and spatio-temporal domain shift.","tags":null,"title":"Dual Encoding U-Net for Spatio-Temporal Domain Shift Frame Prediction","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yyimingucl.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://yyimingucl.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]